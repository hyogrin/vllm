{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# g5/g6 serving performance comparison\n",
    "\n",
    "## [key metrics for LLM serving:](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices)\n",
    "\n",
    "1. **Time To First Token (TTFT)**: How quickly users start seeing the model's output after entering their query. Low waiting times for a response are essential in real-time interactions, but less important in offline workloads. This metric is driven by the time required to process the prompt and then generate the first output token.\n",
    "2. **Time Per Output Token (TPOT)**: Time to generate an output token for *each* user that is querying our system. This metric corresponds with how each user will perceive the \"speed\" of the model. For example, a TPOT of 100 milliseconds/tok would be 10 tokens per second per user, or ~450 words per minute, which is faster than a typical person can read.\n",
    "3. **Latency**: The overall time it takes for the model to generate the full response for a user. Overall response latency can be calculated using the previous two metrics: latency = *(TTFT)* + *(TPOT)* * (the number of tokens to be generated).\n",
    "4. **Throughput**: The number of output tokens per second an inference server can generate across all users and requests.\n",
    "\n",
    "For ease of experimentation, we only measure 1,2,4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up env and start vllm server \n",
    "\n",
    "\n",
    "* ami - [ami-020f2b388c86c9684](https://us-west-2.console.aws.amazon.com/ec2/home?region=us-west-2#Images:visibility=public-images;imageId=ami-020f2b388c86c9684)\n",
    "* watch -n 0.5 -d nvidia-smi\n",
    "* install vllm - https://docs.vllm.ai/en/latest/getting_started/installation.html \n",
    "* open api server - https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n",
    "\n",
    "```\n",
    "# (Recommended) Create a new conda environment.\n",
    "$ conda create -n myenv python=3.9 -y\n",
    "# new terminal\n",
    "$ conda activate myenv\n",
    "$ # Install vLLM with CUDA 12.1.\n",
    "$ pip install vllm===0.3.3\n",
    "```\n",
    "\n",
    "vllm repo git clone\n",
    "\n",
    "```\n",
    "https://github.com/vllm-project/vllm.git\n",
    "```\n",
    "\n",
    "install dependencies\n",
    "\n",
    "```\n",
    "pip install -r requirements-cuda.txt\n",
    "# Install vLLM with CUDA 12.1.\n",
    "pip install vllm==0.3.3\n",
    "#to avoid ModuleNotFoundError: No module named 'vllm._C'\n",
    "cd benchmark \n",
    "```\n",
    "```\n",
    "huggingface-cli login\n",
    "python -m vllm.entrypoints.openai.api_server --model meta-llama/Llama-2-7b-chat-hf --dtype float16 \n",
    "\n",
    "curl http://localhost:8000/v1/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        \"prompt\": \"where is San Francisco?\",\n",
    "        \"max_tokens\": 10,\n",
    "        \"temperature\": 0\n",
    "    }'\n",
    "```\n",
    "\n",
    "```\n",
    "# backend \n",
    "  #  \"tgi\": async_request_tgi,\n",
    "  #  \"vllm\": async_request_openai_completions,\n",
    "  #  \"lmdeploy\": async_request_openai_completions,\n",
    "  #  \"deepspeed-mii\": async_request_deepspeed_mii,\n",
    "  #  \"openai\": async_request_openai_completions,\n",
    "  #  \"openai-chat\": async_request_openai_chat_completions,\n",
    "  #  \"tensorrt-llm\": async_request_trt_llm,\n",
    "    \n",
    "# By default <request_rate> is inf. \n",
    "  # Number of requests per second. If this is inf, \n",
    "  # then all the requests are sent at time 0. \n",
    "  # Otherwise, we use Poisson process to synthesize \n",
    "  # the request arrival times.\n",
    "# By default <num_prompts> is 1000\n",
    "  # Number of prompts to process.\n",
    "# save-result specify to save benchmark results to a json file, action=\"store_true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-11 04:37:28--  https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n",
      "Resolving huggingface.co (huggingface.co)... 99.84.66.112, 99.84.66.70, 99.84.66.72, ...\n",
      "Connecting to huggingface.co (huggingface.co)|99.84.66.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/58/74/5874e8234cbcd37dd31ca486e8492d9f1370bdd04829001f53991a866851e83f/35f0e213ce091ed9b9af2a1f0755e9d39f9ccec34ab281cd4ca60d70f6479ba4?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27ShareGPT_V3_unfiltered_cleaned_split.json%3B+filename%3D%22ShareGPT_V3_unfiltered_cleaned_split.json%22%3B&response-content-type=application%2Fjson&Expires=1713069448&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMzA2OTQ0OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy81OC83NC81ODc0ZTgyMzRjYmNkMzdkZDMxY2E0ODZlODQ5MmQ5ZjEzNzBiZGQwNDgyOTAwMWY1Mzk5MWE4NjY4NTFlODNmLzM1ZjBlMjEzY2UwOTFlZDliOWFmMmExZjA3NTVlOWQzOWY5Y2NlYzM0YWIyODFjZDRjYTYwZDcwZjY0NzliYTQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=W7vwZdn-9LVAUGHD9kyVWLvL4f0a1M1G4EfV29zjdhSM51kGUHD2Pie19cXbOmE3AwaiudGivrfcjSz1VUxGbiXhLCIKXfImQBYjp1dCcgdqp4ax2CefQwIzvV0XK1zMXKjIxAi7E8cS6F3QlIfSz4fECK3JycgszFZ2WGuiSXMQYz9Hx84ES4BjEUJLPlmcEFdxAntgyHfe-igdQQiMUpyvUAOf84MS-CsEPxob3YPNbUArV%7EYl0trVPOpkeaw9crY-suF8NOCYpnoAeswRY21hTqZNaRxiTsk-QbBVIywJ2iJ1FUT5OWOWUJYObPYN-Lh0QguQiTUwBRLa%7E9qQ-A__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2024-04-11 04:37:28--  https://cdn-lfs.huggingface.co/repos/58/74/5874e8234cbcd37dd31ca486e8492d9f1370bdd04829001f53991a866851e83f/35f0e213ce091ed9b9af2a1f0755e9d39f9ccec34ab281cd4ca60d70f6479ba4?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27ShareGPT_V3_unfiltered_cleaned_split.json%3B+filename%3D%22ShareGPT_V3_unfiltered_cleaned_split.json%22%3B&response-content-type=application%2Fjson&Expires=1713069448&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMzA2OTQ0OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy81OC83NC81ODc0ZTgyMzRjYmNkMzdkZDMxY2E0ODZlODQ5MmQ5ZjEzNzBiZGQwNDgyOTAwMWY1Mzk5MWE4NjY4NTFlODNmLzM1ZjBlMjEzY2UwOTFlZDliOWFmMmExZjA3NTVlOWQzOWY5Y2NlYzM0YWIyODFjZDRjYTYwZDcwZjY0NzliYTQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=W7vwZdn-9LVAUGHD9kyVWLvL4f0a1M1G4EfV29zjdhSM51kGUHD2Pie19cXbOmE3AwaiudGivrfcjSz1VUxGbiXhLCIKXfImQBYjp1dCcgdqp4ax2CefQwIzvV0XK1zMXKjIxAi7E8cS6F3QlIfSz4fECK3JycgszFZ2WGuiSXMQYz9Hx84ES4BjEUJLPlmcEFdxAntgyHfe-igdQQiMUpyvUAOf84MS-CsEPxob3YPNbUArV%7EYl0trVPOpkeaw9crY-suF8NOCYpnoAeswRY21hTqZNaRxiTsk-QbBVIywJ2iJ1FUT5OWOWUJYObPYN-Lh0QguQiTUwBRLa%7E9qQ-A__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.161.6.100, 18.161.6.107, 18.161.6.126, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.161.6.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 672837942 (642M) [application/json]\n",
      "Saving to: ‘ShareGPT_V3_unfiltered_cleaned_split.json’\n",
      "\n",
      "ShareGPT_V3_unfilte 100%[===================>] 641.67M   383MB/s    in 1.7s    \n",
      "\n",
      "2024-04-11 04:37:29 (383 MB/s) - ‘ShareGPT_V3_unfiltered_cleaned_split.json’ saved [672837942/672837942]\n",
      "\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/myenv/lib/python3.9/site-packages (from aiohttp) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/myenv/lib/python3.9/site-packages (from aiohttp) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/myenv/lib/python3.9/site-packages (from aiohttp) (1.4.1)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp)\n",
      "  Downloading multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp)\n",
      "  Downloading yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/envs/myenv/lib/python3.9/site-packages (from yarl<2.0,>=1.0->aiohttp) (3.7)\n",
      "Downloading aiohttp-3.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (123 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (304 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.3/304.3 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: multidict, async-timeout, yarl, aiohttp\n",
      "Successfully installed aiohttp-3.9.3 async-timeout-4.0.3 multidict-6.0.5 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "# download sharedgpt for benchmark\n",
    "!wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n",
    "!pip install aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g5.2xlarge serving performance (meta-llama/Llama-2-7b-chat-hf, float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(backend='vllm', base_url=None, host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Llama-2-7b-chat-hf', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=3, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, request_rate=inf, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=True, metadata=None, result_dir=None)\n",
      "Traffic request rate: inf\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:28<00:00,  9.43s/it]\n",
      "============ Serving Benchmark Result ============\n",
      "Successful requests:                     3         \n",
      "Benchmark duration (s):                  28.28     \n",
      "Total input tokens:                      584       \n",
      "Total generated tokens:                  1432      \n",
      "Request throughput (req/s):              0.11      \n",
      "Input token throughput (tok/s):          20.65     \n",
      "Output token throughput (tok/s):         50.64     \n",
      "---------------Time to First Token----------------\n",
      "Mean TTFT (ms):                          92.87     \n",
      "Median TTFT (ms):                        45.44     \n",
      "P99 TTFT (ms):                           185.23    \n",
      "-----Time per Output Token (excl. 1st token)------\n",
      "Mean TPOT (ms):                          30.61     \n",
      "Median TPOT (ms):                        31.25     \n",
      "P99 TPOT (ms):                           31.30     \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# run benchmark_serving request-rate inf, num-prompts 3\n",
    "!python benchmark_serving.py \\\n",
    "        --backend vllm \\\n",
    "        --model \"meta-llama/Llama-2-7b-chat-hf\" \\\n",
    "        --dataset-name sharegpt \\\n",
    "        --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \\\n",
    "        --request-rate inf \\\n",
    "        --num-prompts 3 \\\n",
    "        --save-result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(backend='vllm', base_url=None, host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Llama-2-7b-chat-hf', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, request_rate=inf, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None)\n",
      "Traffic request rate: inf\n",
      "100%|█████████████████████████████████████████| 100/100 [00:48<00:00,  2.08it/s]\n",
      "============ Serving Benchmark Result ============\n",
      "Successful requests:                     100       \n",
      "Benchmark duration (s):                  48.04     \n",
      "Total input tokens:                      25900     \n",
      "Total generated tokens:                  18393     \n",
      "Request throughput (req/s):              2.08      \n",
      "Input token throughput (tok/s):          539.11    \n",
      "Output token throughput (tok/s):         382.85    \n",
      "---------------Time to First Token----------------\n",
      "Mean TTFT (ms):                          4586.31   \n",
      "Median TTFT (ms):                        4531.84   \n",
      "P99 TTFT (ms):                           12009.92  \n",
      "-----Time per Output Token (excl. 1st token)------\n",
      "Mean TPOT (ms):                          120.50    \n",
      "Median TPOT (ms):                        79.76     \n",
      "P99 TPOT (ms):                           666.99    \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# run benchmark_serving request-rate inf, num-prompts 100\n",
    "!python benchmark_serving.py \\\n",
    "        --backend vllm \\\n",
    "        --model \"meta-llama/Llama-2-7b-chat-hf\" \\\n",
    "        --dataset-name sharegpt \\\n",
    "        --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \\\n",
    "        --request-rate inf \\\n",
    "        --num-prompts 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(backend='vllm', base_url=None, host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Llama-2-7b-chat-hf', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, request_rate=10.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None)\n",
      "Traffic request rate: 10.0\n",
      "100%|█████████████████████████████████████████| 100/100 [00:48<00:00,  2.06it/s]\n",
      "============ Serving Benchmark Result ============\n",
      "Successful requests:                     100       \n",
      "Benchmark duration (s):                  48.43     \n",
      "Total input tokens:                      25900     \n",
      "Total generated tokens:                  18432     \n",
      "Request throughput (req/s):              2.06      \n",
      "Input token throughput (tok/s):          534.74    \n",
      "Output token throughput (tok/s):         380.55    \n",
      "---------------Time to First Token----------------\n",
      "Mean TTFT (ms):                          512.23    \n",
      "Median TTFT (ms):                        275.92    \n",
      "P99 TTFT (ms):                           2708.13   \n",
      "-----Time per Output Token (excl. 1st token)------\n",
      "Mean TPOT (ms):                          99.08     \n",
      "Median TPOT (ms):                        77.29     \n",
      "P99 TPOT (ms):                           379.88    \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# run benchmark_serving request-rate 10, num-prompts 100\n",
    "!python benchmark_serving.py \\\n",
    "        --backend vllm \\\n",
    "        --model \"meta-llama/Llama-2-7b-chat-hf\" \\\n",
    "        --dataset-name sharegpt \\\n",
    "        --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \\\n",
    "        --request-rate 10 \\\n",
    "        --num-prompts 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(backend='vllm', base_url=None, host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Llama-2-7b-chat-hf', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, request_rate=100.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None)\n",
      "Traffic request rate: 100.0\n",
      "100%|█████████████████████████████████████████| 100/100 [00:47<00:00,  2.12it/s]\n",
      "============ Serving Benchmark Result ============\n",
      "Successful requests:                     100       \n",
      "Benchmark duration (s):                  47.13     \n",
      "Total input tokens:                      25900     \n",
      "Total generated tokens:                  18184     \n",
      "Request throughput (req/s):              2.12      \n",
      "Input token throughput (tok/s):          549.60    \n",
      "Output token throughput (tok/s):         385.87    \n",
      "---------------Time to First Token----------------\n",
      "Mean TTFT (ms):                          3506.52   \n",
      "Median TTFT (ms):                        3214.50   \n",
      "P99 TTFT (ms):                           10373.83  \n",
      "-----Time per Output Token (excl. 1st token)------\n",
      "Mean TPOT (ms):                          117.36    \n",
      "Median TPOT (ms):                        76.95     \n",
      "P99 TPOT (ms):                           460.40    \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# run benchmark_serving request-rate 100, num-prompts 100\n",
    "!python benchmark_serving.py \\\n",
    "        --backend vllm \\\n",
    "        --model \"meta-llama/Llama-2-7b-chat-hf\" \\\n",
    "        --dataset-name sharegpt \\\n",
    "        --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \\\n",
    "        --request-rate 100 \\\n",
    "        --num-prompts 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(backend='vllm', base_url=None, host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Llama-2-7b-chat-hf', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=1000, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, request_rate=100.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None)\n",
      "Traffic request rate: 100.0\n",
      "100%|███████████████████████████████████████| 1000/1000 [06:40<00:00,  2.49it/s]\n",
      "============ Serving Benchmark Result ============\n",
      "Successful requests:                     1000      \n",
      "Benchmark duration (s):                  400.92    \n",
      "Total input tokens:                      248339    \n",
      "Total generated tokens:                  195161    \n",
      "Request throughput (req/s):              2.49      \n",
      "Input token throughput (tok/s):          619.42    \n",
      "Output token throughput (tok/s):         486.78    \n",
      "---------------Time to First Token----------------\n",
      "Mean TTFT (ms):                          144667.87 \n",
      "Median TTFT (ms):                        133582.83 \n",
      "P99 TTFT (ms):                           364190.00 \n",
      "-----Time per Output Token (excl. 1st token)------\n",
      "Mean TPOT (ms):                          90.75     \n",
      "Median TPOT (ms):                        79.91     \n",
      "P99 TPOT (ms):                           353.55    \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# run benchmark_serving request-rate 100, num-prompts 1000\n",
    "!python benchmark_serving.py \\\n",
    "        --backend vllm \\\n",
    "        --model \"meta-llama/Llama-2-7b-chat-hf\" \\\n",
    "        --dataset-name sharegpt \\\n",
    "        --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \\\n",
    "        --request-rate 100 \\\n",
    "        --num-prompts 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g6.2xlarge serving performance (meta-llama/Llama-2-7b-chat-hf, float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(backend='vllm', base_url=None, host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Llama-2-7b-chat-hf', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=3, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, request_rate=inf, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=True, metadata=None, result_dir=None)\n",
      "Traffic request rate: inf\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:50<00:00, 16.73s/it]\n",
      "============ Serving Benchmark Result ============\n",
      "Successful requests:                     3         \n",
      "Benchmark duration (s):                  50.18     \n",
      "Total input tokens:                      584       \n",
      "Total generated tokens:                  1432      \n",
      "Request throughput (req/s):              0.06      \n",
      "Input token throughput (tok/s):          11.64     \n",
      "Output token throughput (tok/s):         28.54     \n",
      "---------------Time to First Token----------------\n",
      "Mean TTFT (ms):                          130.63    \n",
      "Median TTFT (ms):                        76.07     \n",
      "P99 TTFT (ms):                           236.81    \n",
      "-----Time per Output Token (excl. 1st token)------\n",
      "Mean TPOT (ms):                          54.76     \n",
      "Median TPOT (ms):                        55.55     \n",
      "P99 TPOT (ms):                           56.20     \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# run benchmark_serving request-rate inf, num-prompts 3\n",
    "!python benchmark_serving.py \\\n",
    "        --backend vllm \\\n",
    "        --model \"meta-llama/Llama-2-7b-chat-hf\" \\\n",
    "        --dataset-name sharegpt \\\n",
    "        --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \\\n",
    "        --request-rate inf \\\n",
    "        --num-prompts 3 \\\n",
    "        --save-result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(backend='vllm', base_url=None, host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Llama-2-7b-chat-hf', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, request_rate=inf, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None)\n",
      "Traffic request rate: inf\n",
      "100%|█████████████████████████████████████████| 100/100 [01:18<00:00,  1.28it/s]\n",
      "============ Serving Benchmark Result ============\n",
      "Successful requests:                     100       \n",
      "Benchmark duration (s):                  78.32     \n",
      "Total input tokens:                      25900     \n",
      "Total generated tokens:                  18268     \n",
      "Request throughput (req/s):              1.28      \n",
      "Input token throughput (tok/s):          330.69    \n",
      "Output token throughput (tok/s):         233.25    \n",
      "---------------Time to First Token----------------\n",
      "Mean TTFT (ms):                          5622.48   \n",
      "Median TTFT (ms):                        5132.68   \n",
      "P99 TTFT (ms):                           17272.15  \n",
      "-----Time per Output Token (excl. 1st token)------\n",
      "Mean TPOT (ms):                          166.96    \n",
      "Median TPOT (ms):                        119.58    \n",
      "P99 TPOT (ms):                           590.05    \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# run benchmark_serving request-rate inf, num-prompts 100\n",
    "!python benchmark_serving.py \\\n",
    "        --backend vllm \\\n",
    "        --model \"meta-llama/Llama-2-7b-chat-hf\" \\\n",
    "        --dataset-name sharegpt \\\n",
    "        --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \\\n",
    "        --request-rate inf \\\n",
    "        --num-prompts 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(backend='vllm', base_url=None, host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Llama-2-7b-chat-hf', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, request_rate=10.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None)\n",
      "Traffic request rate: 10.0\n",
      "100%|█████████████████████████████████████████| 100/100 [01:18<00:00,  1.27it/s]\n",
      "============ Serving Benchmark Result ============\n",
      "Successful requests:                     100       \n",
      "Benchmark duration (s):                  78.86     \n",
      "Total input tokens:                      25900     \n",
      "Total generated tokens:                  18510     \n",
      "Request throughput (req/s):              1.27      \n",
      "Input token throughput (tok/s):          328.43    \n",
      "Output token throughput (tok/s):         234.72    \n",
      "---------------Time to First Token----------------\n",
      "Mean TTFT (ms):                          1169.02   \n",
      "Median TTFT (ms):                        522.90    \n",
      "P99 TTFT (ms):                           7814.95   \n",
      "-----Time per Output Token (excl. 1st token)------\n",
      "Mean TPOT (ms):                          155.74    \n",
      "Median TPOT (ms):                        119.80    \n",
      "P99 TPOT (ms):                           478.67    \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# run benchmark_serving request-rate 10, num-prompts 100\n",
    "!python benchmark_serving.py \\\n",
    "        --backend vllm \\\n",
    "        --model \"meta-llama/Llama-2-7b-chat-hf\" \\\n",
    "        --dataset-name sharegpt \\\n",
    "        --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \\\n",
    "        --request-rate 10 \\\n",
    "        --num-prompts 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(backend='vllm', base_url=None, host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Llama-2-7b-chat-hf', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, request_rate=100.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None)\n",
      "Traffic request rate: 100.0\n",
      "100%|█████████████████████████████████████████| 100/100 [01:17<00:00,  1.29it/s]\n",
      "============ Serving Benchmark Result ============\n",
      "Successful requests:                     100       \n",
      "Benchmark duration (s):                  77.24     \n",
      "Total input tokens:                      25900     \n",
      "Total generated tokens:                  18365     \n",
      "Request throughput (req/s):              1.29      \n",
      "Input token throughput (tok/s):          335.33    \n",
      "Output token throughput (tok/s):         237.77    \n",
      "---------------Time to First Token----------------\n",
      "Mean TTFT (ms):                          4406.23   \n",
      "Median TTFT (ms):                        3921.46   \n",
      "P99 TTFT (ms):                           13990.11  \n",
      "-----Time per Output Token (excl. 1st token)------\n",
      "Mean TPOT (ms):                          164.19    \n",
      "Median TPOT (ms):                        117.71    \n",
      "P99 TPOT (ms):                           570.07    \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# run benchmark_serving request-rate 100, num-prompts 100\n",
    "!python benchmark_serving.py \\\n",
    "        --backend vllm \\\n",
    "        --model \"meta-llama/Llama-2-7b-chat-hf\" \\\n",
    "        --dataset-name sharegpt \\\n",
    "        --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \\\n",
    "        --request-rate 100 \\\n",
    "        --num-prompts 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(backend='vllm', base_url=None, host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Llama-2-7b-chat-hf', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=1000, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, request_rate=100.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None)\n",
      "Traffic request rate: 100.0\n",
      "100%|███████████████████████████████████████| 1000/1000 [10:33<00:00,  1.58it/s]\n",
      "============ Serving Benchmark Result ============\n",
      "Successful requests:                     1000      \n",
      "Benchmark duration (s):                  633.90    \n",
      "Total input tokens:                      248339    \n",
      "Total generated tokens:                  195463    \n",
      "Request throughput (req/s):              1.58      \n",
      "Input token throughput (tok/s):          391.76    \n",
      "Output token throughput (tok/s):         308.35    \n",
      "---------------Time to First Token----------------\n",
      "Mean TTFT (ms):                          225627.60 \n",
      "Median TTFT (ms):                        203008.89 \n",
      "P99 TTFT (ms):                           570937.13 \n",
      "-----Time per Output Token (excl. 1st token)------\n",
      "Mean TPOT (ms):                          140.84    \n",
      "Median TPOT (ms):                        124.75    \n",
      "P99 TPOT (ms):                           577.19    \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# run benchmark_serving request-rate 100, num-prompts 1000\n",
    "!python benchmark_serving.py \\\n",
    "        --backend vllm \\\n",
    "        --model \"meta-llama/Llama-2-7b-chat-hf\" \\\n",
    "        --dataset-name sharegpt \\\n",
    "        --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \\\n",
    "        --request-rate 100 \\\n",
    "        --num-prompts 1000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
